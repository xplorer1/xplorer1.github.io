---
layout: page
title: Cognitive Offloading in AI-Assisted Coding
description: Self-Experiment on AI-Assisted versus Manual Coding Effects
img: assets/img/12.jpg
importance: 1
category: research
related_publications: true
---

## Self-Experiment on AI-Assisted versus Manual Coding: Effects on Cognitive Offloading and Critical Thinking

This research project documents a week-long self-experiment designed to compare AI-assisted coding against manual coding under controlled conditions. The study investigates the trade-offs between productivity gains and potential cognitive costs when using generative AI tools in software development.

### Research Question

**How does AI-assisted coding affect cognitive offloading, critical thinking, and learning outcomes compared to manual coding approaches?**

### Experimental Design

The experiment spanned seven consecutive days with alternating conditions:
- **AI-Assisted Days** (Days 1, 3, 5, 7): Used Cursor AI coding assistant with enforced reflection
- **Manual Coding Days** (Days 2, 4, 6): Traditional coding without AI assistance
- **Controlled Variables**: Comparable algorithmic and debugging problems from LeetCode

### Key Findings

Our controlled experiment revealed important insights about AI-assisted development:

#### Performance Metrics
- **Time to Completion**: AI assistance reduced completion time by ~35% (30 min vs 45 min average)
- **Error Rates**: AI-assisted coding showed lower error rates (5.0% vs 8.5% average)
- **Reproducibility**: Manual coding achieved higher reproducibility scores (0.85 vs 0.73 average)
- **Reflection Quality**: Both conditions maintained high reflection quality (4.2-4.7 range)

#### Cognitive Impact
- **Productivity Gains**: AI tools significantly accelerate problem-solving
- **Cognitive Offloading**: Some delegation of thinking to external aids
- **Reflection Mitigation**: Enforced reflection helps maintain critical engagement
- **Learning Retention**: Manual coding shows better long-term solution retention

### Methodology

#### Data Collection
- **Quantitative Metrics**: Time, error rates, reproducibility scores
- **Qualitative Assessment**: Reflection quality evaluation using ChatGPT-5 rubric
- **Controlled Conditions**: Equivalent problem sets across different days
- **24-Hour Delay Testing**: Reproducibility assessment after time delay

#### Reflection Protocol
- **AI-Assisted Days**: Required written explanation of AI suggestions before use
- **Manual Days**: Self-explanation of solution approaches
- **Quality Assessment**: Standardized rubric for reflection evaluation
- **Critical Thinking**: Emphasis on understanding rather than just implementation

### Results Analysis

The data shows clear trade-offs between AI assistance and manual coding:

| Metric | AI-Assisted | Manual | Improvement |
|--------|-------------|---------|-------------|
| Time (min) | 30 | 45 | +33% faster |
| Error Rate (%) | 5.0 | 8.5 | -41% errors |
| Reproducibility | 0.73 | 0.85 | -14% retention |
| Reflection Quality | 4.2 | 4.6 | Comparable |

### Educational Implications

This research provides valuable insights for educational institutions:

#### Integration Strategies
- **Hybrid Approaches**: Combine AI tools with reflection requirements
- **Balanced Practice**: Alternate between AI-assisted and manual coding
- **Academic Integrity**: Maintain rigor while leveraging productivity benefits
- **Critical Engagement**: Use AI as a tool for learning, not replacement

#### Assessment Design
- **Reflection Requirements**: Mandate explanation of AI-generated solutions
- **Mixed Modalities**: Vary between assisted and independent problem-solving
- **Long-term Retention**: Test understanding beyond immediate implementation
- **Critical Evaluation**: Assess ability to critique and improve AI suggestions

### Technologies Used

- **Cursor AI**: Primary AI coding assistant
- **LeetCode**: Algorithmic problem platform
- **ChatGPT-5**: Reflection quality assessment
- **Statistical Analysis**: Performance metrics and trend analysis
- **Experimental Design**: Controlled A/B testing methodology

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/pdf/Cognitive_Offloading.pdf" title="Full Research Paper" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Download the complete research paper for detailed methodology, results, and analysis.
</div>

### Future Work

This experiment opens several research directions:
- **Larger Sample Sizes**: Extend to multiple participants and longer durations
- **Reflection Protocols**: Optimize reflection methods for maximum learning benefit
- **Different AI Tools**: Compare various coding assistants and their effects
- **Longitudinal Studies**: Track long-term learning outcomes and skill development
- **Industry Applications**: Study effects in professional software development contexts

### Impact on AI Education

This work contributes to the critical conversation about AI integration in education:
- **Balanced Approach**: Demonstrates how to use AI responsibly in learning
- **Cognitive Awareness**: Highlights the importance of maintaining critical thinking
- **Best Practices**: Provides evidence-based guidelines for AI-assisted education
- **Future Preparation**: Prepares students for AI-enhanced professional environments

The project demonstrates that AI tools can enhance productivity while maintaining educational rigor when properly integrated with reflection and critical thinking requirements.
